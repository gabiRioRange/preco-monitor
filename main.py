from config import URLS_ALVO, ARQUIVO_SAIDA, logger
from src.scraper import obter_dados_produto
from src.cleaner import limpar_dados
from src.exporter import salvar_excel
import time

def main():
    logger.info("--- üöÄ Iniciando Monitoramento de Pre√ßos ---")
    
    dados_coletados = []
    
    # 1. Coleta (Scraping)
    for item in URLS_ALVO:
        dados = obter_dados_produto(item)
        if dados:
            dados_coletados.append(dados)
        # Pausa para n√£o sobrecarregar o servidor
        time.sleep(2)
            
    # 2. Limpeza (ETL)
    if dados_coletados:
        logger.info("üßπ Limpando e estruturando dados...")
        df_limpo = limpar_dados(dados_coletados)
        
        # Mostra pr√©via no console (CORRIGIDO AQUI: Pre√ßo Atual)
        logger.info("\nPr√©via dos dados:")
        try:
            # Tenta mostrar as colunas novas
            logger.info(str(df_limpo[['Produto', 'Pre√ßo Atual']].head()))
        except KeyError:
            # Se der erro, mostra tudo o que tem
            logger.info(str(df_limpo.head()))
        
        # 3. Exporta√ß√£o
        logger.info(f"\nüíæ Salvando em {ARQUIVO_SAIDA}...")
        salvar_excel(df_limpo, ARQUIVO_SAIDA)
        
    else:
        logger.warning("‚ö†Ô∏è Nenhum dado foi coletado. Verifique os seletores ou a conex√£o.")

    logger.info("--- ‚úÖ Processo Finalizado ---")

if __name__ == "__main__":
    main()